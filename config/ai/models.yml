# ============================================================================
# MCP v7 â€” LiteLLM Model Configuration
# ============================================================================
# Routes AI requests to the correct local model via Ollama.
# Fallback-Konfiguration: Bei Ausfall des primaeren Modells wird das
# naechste Modell in der Liste verwendet (LiteLLM Fallback-Logik).
# ============================================================================

model_list:
  - model_name: mistral
    litellm_params:
      model: ollama/mistral:7b-instruct-v0.3-q4_K_M
      api_base: http://ollama:11434
      stream: true
      timeout: 120
      max_retries: 2

  - model_name: llama3
    litellm_params:
      model: ollama/llama3:8b-instruct-q4_K_M
      api_base: http://ollama:11434
      stream: true
      timeout: 120
      max_retries: 2

  - model_name: nomic-embed
    litellm_params:
      model: ollama/nomic-embed-text
      api_base: http://ollama:11434
      timeout: 60

general_settings:
  master_key: ${LITELLM_MASTER_KEY}

router_settings:
  routing_strategy: "simple-shuffle"
  num_retries: 2
  timeout: 120
  allowed_fails: 3
  cooldown_time: 60
