# MCP v7 ‚Äî Vollst√§ndige Code-Analyse & Optimierungsplan

## üî¥ KRITISCH: Funktionen die NUR als Name existieren (ohne Implementierung)

### 1. AI Gateway ‚Äî `/api/v1/search` (Zeile 223-238, `main.py`)
- **Status:** STUB ‚Äî gibt nur Placeholder zur√ºck
- **Code:** `"message": "RAG search ‚Äî pgvector integration pending"`
- **Fehlt:** pgvector-Verbindung, Vektor-Suche, Embedding-Vergleich

### 2. AI Gateway ‚Äî `/api/v1/embed` (Zeile 198-220, `main.py`)
- **Status:** HALBFERTIG ‚Äî erzeugt Embeddings via Ollama, speichert sie aber NIRGENDS
- **Fehlt:** Speicherung in pgvector, Metadaten-Handling, Collection-Management

### 3. LangChain Worker ‚Äî RAG-Pipeline (komplett fehlend)
- **Status:** NICHT IMPLEMENTIERT ‚Äî trotz Name "LangChain Worker"
- **Worker** nutzt LangChain √ºberhaupt NICHT (nur raw httpx)
- **Fehlt:** pgvector-Verbindung, Embedding-Suche, RAG-Context-Injection
- **Konfiguriert in** `rag-config.yml` aber NICHT im Code verwendet

### 4. LangChain Worker ‚Äî Zammad Ticket-Erstellung (fehlend)
- **Status:** NICHT IMPLEMENTIERT
- **Beschrieben im Docstring** (Zeile 5: "Create ticket in Zammad") aber Code fehlt
- **Fehlt:** HTTP-Call zu Zammad API, Ticket-Mapping, Priority-Handling

### 5. LangChain Worker ‚Äî ntfy Benachrichtigung (fehlend)
- **Status:** NICHT IMPLEMENTIERT
- **Beschrieben im Docstring** (Zeile 6: "Send notification via ntfy") aber Code fehlt
- **Fehlt:** ntfy POST-Request, Channel-Konfiguration, Severity-basiertes Routing

### 6. LangChain Worker ‚Äî Professioneller Prompt nicht verwendet
- **Status:** IGNORIERT
- **Datei** `config/ai/prompts/alert-analysis.txt` enth√§lt professionellen Prompt mit RAG-Slots
- **Worker** nutzt stattdessen einen simplen hardcodierten Prompt (Zeile 67-83)
- **Prompt-Datei** hat Felder f√ºr: `{rag_results}`, `{crowdsec_alerts}`, `{metrics}` ‚Äî alle ungenutzt

### 7. LiteLLM ‚Äî konfiguriert aber ungenutzt
- **Status:** VERSCHWENDUNG ‚Äî Container l√§uft, wird aber nie angesprochen
- **Worker** spricht direkt mit Ollama statt √ºber LiteLLM
- **LiteLLM** sollte als Model-Router fungieren (Failover, Load-Balancing)

---

## üü° Fehlende Service-Verbindungen (Service-Mesh-L√ºcken)

| Von ‚Üí Nach | Status | Beschreibung |
|---|---|---|
| **Zabbix ‚Üí AI Gateway** | ‚ùå FEHLT | Kein Webhook/Trigger konfiguriert. Zabbix-Alerts erreichen die AI nie |
| **CrowdSec ‚Üí AI Gateway** | ‚ùå FEHLT | IDS-Daten werden nicht an AI weitergeleitet |
| **n8n ‚Üí AI Gateway** | ‚ùå FEHLT | Keine Workflows definiert (leerer Ordner `config/n8n/workflows/`) |
| **n8n ‚Üí Zabbix** | ‚ùå FEHLT | Keine Automatisierung |
| **n8n ‚Üí Zammad** | ‚ùå FEHLT | Keine Automatisierung |
| **n8n ‚Üí ntfy** | ‚ùå FEHLT | Keine Automatisierung |
| **AI Gateway ‚Üí LiteLLM** | ‚ùå FEHLT | Gateway spricht nicht mit LiteLLM |
| **LangChain ‚Üí pgvector** | ‚ùå FEHLT | Env-Vars konfiguriert aber kein Code |
| **LangChain ‚Üí Zammad** | ‚ùå FEHLT | Docstring beschreibt es, Code fehlt |
| **LangChain ‚Üí ntfy** | ‚ùå FEHLT | Docstring beschreibt es, Code fehlt |
| **Keycloak ‚Üí alle Services** | ‚ùå FEHLT | SSO nirgends integriert |
| **OpenBao ‚Üí alle Services** | ‚ùå FEHLT | Secrets werden per ENV verwaltet, nicht √ºber Vault |
| **BookStack ‚Üí RAG** | ‚ùå FEHLT | Wissensbasis wird nicht f√ºr RAG genutzt |
| **Grafana ‚Üí AI Metrics** | ‚ùå FEHLT | Kein Dashboard f√ºr AI-Pipeline-Metriken |
| **Alloy ‚Üí AI Logs** | ‚úÖ OK | Docker-Logs werden via Alloy ‚Üí Loki gesammelt |
| **Grafana ‚Üí Loki** | ‚úÖ OK | Datasource konfiguriert |
| **Grafana ‚Üí Zabbix** | ‚úÖ OK | Datasource konfiguriert |

---

## üü¢ Was funktioniert

- Docker-Compose-Architektur (5 Stacks, 5 Netzwerke)
- Nginx Reverse-Proxy mit 13 korrekten Proxy-Pfaden
- Dashboard HTML
- Redis-Queue + Deduplication in AI Gateway
- Ollama-Analyse (simpel aber funktional)
- Job-Queue-System (Redis LPUSH/BRPOP)
- Health-Checks f√ºr alle Container
- Alloy ‚Üí Loki Log-Pipeline
- Grafana Datasource-Provisioning
- Security-Hardening (no-new-privileges, network isolation)
- Smoke/AI/Security Tests

---

## üü† Fehlende Dateien (im Makefile referenziert)

- `scripts/mcp-backup.sh` ‚Äî existiert NICHT
- `scripts/mcp-restore.sh` ‚Äî existiert NICHT
- `scripts/mcp-pull-images.sh` ‚Äî existiert NICHT

---

## üìä Zusammenfassung der L√ºcken

| Kategorie | Anzahl |
|---|---|
| Stub/Placeholder Endpoints | 2 |
| Fehlende Kernfunktionen im Worker | 4 |
| Ungenutzte Services | 2 (LiteLLM, OpenBao) |
| Fehlende Service-Verbindungen | 12 |
| Fehlende Dateien | 3 |
| **Gesamt** | **23 L√ºcken** |

---
---
---

# ü§ñ Claude Code Prompt ‚Äî MCP v7 Vollst√§ndige Optimierung

Kopiere den folgenden Prompt in Claude Code und f√ºhre ihn aus:

---

```
Du bist der Programmier-Assistent f√ºr das Masdor MCP v7 Projekt (GitHub Repo).
Ich bin Moustafa, der Admin.

## KONTEXT
MCP v7 ist eine AI-powered IT Operations Platform mit 33 Containern in 5 Docker-Compose-Stacks.
Die Architektur steht, aber viele Funktionen sind nur als Name/Stub implementiert.
Ich brauche dich, um ALLE folgenden L√ºcken systematisch zu schlie√üen.

## REGELN
- Arbeite Phase f√ºr Phase, commit nach jeder Phase
- Teste jede √Ñnderung bevor du weitergehst
- Bestehenden funktionierenden Code NICHT brechen
- Deutsche Kommentare im Code, deutsche AI-Prompts
- Alle Konfiguration √ºber Environment-Variablen
- Keine hartcodierten Secrets im Code

## PHASE 1: LangChain Worker komplett neu implementieren
Datei: `containers/langchain-worker/app/worker.py`

### 1.1 pgvector-Integration einbauen
- Verbindung zu pgvector herstellen (Env-Vars sind schon in docker-compose)
- Tabelle `mcp_knowledge` erstellen: id, embedding vector(768), text, metadata jsonb, created_at
- Bei jedem Job: RAG-Suche mit Embedding-Vergleich (top_k=5, threshold=0.7)
- Konfiguration aus `config/ai/rag-config.yml` laden

### 1.2 Professionellen Prompt verwenden
- Prompt aus `config/ai/prompts/alert-analysis.txt` laden statt hardcoded
- Alle Platzhalter bef√ºllen: {alert_type}, {source}, {hostname}, {severity}, {timestamp}, {description}, {metrics}, {logs}, {crowdsec_alerts}, {rag_results}
- RAG-Ergebnisse als Kontext einsetzen

### 1.3 LiteLLM statt direkt Ollama verwenden
- Worker soll √ºber LiteLLM (http://litellm:4000) statt direkt √ºber Ollama kommunizieren
- OpenAI-kompatibles API-Format verwenden (LiteLLM bietet das)
- Failover-Logik: wenn LiteLLM nicht erreichbar ‚Üí Fallback auf Ollama direkt

### 1.4 Zammad Ticket-Erstellung implementieren
- Nach erfolgreicher Analyse: Ticket in Zammad erstellen via REST API
- POST zu http://zammad-rails:3000/api/v1/tickets
- Token aus ZAMMAD_TOKEN env var (wird √ºber AI Gateway weitergereicht)
- Ticket-Felder: title, group, priority (aus AI-Analyse), note (AI-Bericht)
- Job-ID als Referenz im Ticket

### 1.5 ntfy Benachrichtigung implementieren
- Nach Ticket-Erstellung: Push-Notification √ºber ntfy
- POST zu http://ntfy:80/mcp-alerts
- Severity ‚Üí Priority-Mapping: Kritisch=5, Hoch=4, Mittel=3, Gering=2
- Inhalt: Ticket-Titel, Impact, Sofortma√ünahme, Link zum Ticket

### 1.6 LangChain tats√§chlich verwenden
- LangChain Chains f√ºr die RAG-Pipeline nutzen
- LangChain pgvector VectorStore einbinden
- LangChain Prompt Templates verwenden

## PHASE 2: AI Gateway Endpoints fertigstellen
Datei: `containers/ai-gateway/app/main.py`

### 2.1 `/api/v1/search` ‚Äî RAG-Suche implementieren
- pgvector-Verbindung herstellen (psycopg2 + pgvector Extension)
- Query-Text ‚Üí Embedding via Ollama
- Vektor-√Ñhnlichkeitssuche in pgvector
- Ergebnisse mit Score zur√ºckgeben
- Parameter: query, top_k, similarity_threshold

### 2.2 `/api/v1/embed` ‚Äî Speicherung erg√§nzen
- Nach Embedding-Erstellung: in pgvector speichern
- Metadaten als JSONB speichern
- Duplikat-Check via Text-Hash
- Response: embedding_id, dimensions, stored=true

### 2.3 Neue Endpoints hinzuf√ºgen
- `GET /api/v1/jobs/{job_id}` ‚Äî Job-Status abfragen
- `GET /api/v1/jobs` ‚Äî Alle Jobs listen (mit Pagination)
- `POST /api/v1/ingest` ‚Äî Dokument f√ºr RAG aufnehmen (Chunking + Embedding)
- `GET /api/v1/models` ‚Äî Verf√ºgbare Modelle anzeigen (via LiteLLM)
- `DELETE /api/v1/knowledge/{id}` ‚Äî RAG-Eintrag l√∂schen

### 2.4 Metriken erweitern
- Prometheus-Format f√ºr `/metrics` (nicht nur JSON)
- Z√§hler: requests_total, analyses_completed, tickets_created, rag_searches, embeddings_stored
- Histogramm: analysis_duration_seconds
- Gauge: queue_length, active_jobs

### 2.5 Health-Check erweitern
- Pr√ºfe alle Abh√§ngigkeiten: Redis, Ollama, LiteLLM, pgvector, Zammad, ntfy
- Jede Komponente einzeln mit Status

## PHASE 3: n8n Workflows erstellen
Verzeichnis: `config/n8n/workflows/`

Erstelle JSON-Workflow-Dateien f√ºr:

### W1: Zabbix ‚Üí AI Gateway
- Trigger: Zabbix Webhook empfangen
- Transform: Zabbix-Alert-Format ‚Üí AI Gateway AnalyzeRequest
- Action: POST an http://ai-gateway:8000/api/v1/analyze
- Error-Handling: Retry 3x, dann ntfy-Alert

### W2: CrowdSec ‚Üí AI Gateway
- Trigger: CrowdSec Alert via HTTP
- Transform: CrowdSec-Daten ‚Üí crowdsec_alerts Feld
- Action: POST an AI Gateway mit Security-Severity

### W3: BookStack ‚Üí RAG Ingestion
- Trigger: Webhook bei neuer/ge√§nderter BookStack-Seite
- Action: Text extrahieren, POST an /api/v1/ingest
- Damit wird die Wissensbasis automatisch bef√ºllt

### W4: Periodischer Health-Report
- Trigger: Cron (t√§glich 8:00)
- Sammle: Uptime Kuma Status, Zabbix Probleme, offene Tickets
- Generiere: AI-Zusammenfassung
- Sende: ntfy Daily Report

## PHASE 4: Service-Integrationen verbessern

### 4.1 Grafana Dashboard f√ºr AI-Pipeline
- Erstelle JSON-Dashboard-Datei: `config/grafana/dashboards/ai-pipeline.json`
- Panels: Queue-L√§nge, Analyse-Dauer, Tickets erstellt, Konfidenz-Verteilung
- Datenquelle: AI Gateway /metrics + Loki Logs

### 4.2 Grafana Dashboard Provisioning
- Erstelle `config/grafana/dashboard-provider.yml` f√ºr automatisches Dashboard-Loading

### 4.3 pgvector Initialisierung
- Erweitere `scripts/init-db.sh` um pgvector-Setup:
  - CREATE EXTENSION vector;
  - CREATE TABLE mcp_knowledge (id, embedding, text, metadata, created_at)
  - CREATE INDEX f√ºr Vektor-√Ñhnlichkeitssuche (ivfflat oder hnsw)

## PHASE 5: Fehlende Scripts erstellen

### 5.1 `scripts/mcp-backup.sh`
- Backup aller Docker-Volumes
- PostgreSQL pg_dump f√ºr alle DBs
- pgvector Daten
- Komprimiertes tar.gz mit Timestamp

### 5.2 `scripts/mcp-restore.sh`
- Interaktiv: Backup-Datei ausw√§hlen
- Restore-Reihenfolge: DB ‚Üí Volumes ‚Üí Verify

### 5.3 `scripts/mcp-pull-images.sh`
- Alle Images aus docker-compose Dateien extrahieren
- Parallel pullen mit Progress

## PHASE 6: Container-Optimierungen

### 6.1 Dockerfiles verbessern
- Multi-stage Builds f√ºr kleinere Images
- Non-root User
- Health-Check in Dockerfile
- .dockerignore Dateien erstellen

### 6.2 requirements.txt aufr√§umen
- Ungenutzte Packages entfernen
- Version-Pinning √ºberpr√ºfen

## ABSCHLUSS
- Alle Tests (make test) m√ºssen bestehen
- `make status` muss alle Container als healthy zeigen
- Dokumentation in README aktualisieren
- CHANGELOG.md erstellen

Beginne mit Phase 1. Zeige mir zuerst den Plan f√ºr die worker.py Neuimplementierung, dann implementiere sie Schritt f√ºr Schritt.
```

---

# Zusammenfassung der 6 Phasen

| Phase | Beschreibung | Dateien | Priorit√§t |
|---|---|---|---|
| **Phase 1** | LangChain Worker komplett | `worker.py` | üî¥ Kritisch |
| **Phase 2** | AI Gateway Endpoints | `main.py` | üî¥ Kritisch |
| **Phase 3** | n8n Workflows | `config/n8n/workflows/` | üü° Hoch |
| **Phase 4** | Grafana + pgvector Init | `config/grafana/`, `init-db.sh` | üü° Hoch |
| **Phase 5** | Fehlende Scripts | `scripts/` | üü† Mittel |
| **Phase 6** | Container-Optimierungen | `Dockerfile`, `requirements.txt` | üü† Mittel |
