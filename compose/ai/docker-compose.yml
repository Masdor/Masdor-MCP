# ============================================================================
# MCP v7 — AI Stack (5 Containers: #28-#32)
# ============================================================================
# Ollama, LiteLLM, LangChain Worker, AI Gateway, Redis Queue
# ============================================================================
# RULE: AI Gateway NEVER exposed publicly — internal only
# RULE: Two separate Redis instances (mcp-redis = cache, mcp-redis-queue = AI jobs)
# ============================================================================

# Shared logging config (YAML anchor)
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

services:

  # --------------------------------------------------------------------------
  # #28 — Ollama (LLM Inference — GPU)
  # --------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:${OLLAMA_TAG:-latest}
    container_name: mcp-ollama
    restart: unless-stopped
    networks:
      - mcp-ai-net
    expose:
      - "11434"
    volumes:
      - mcp-ollama-data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 15
      start_period: 300s
    # GPU passthrough (uncomment if NVIDIA GPU available)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: "4.0"
    logging: *default-logging
    security_opt:
      - no-new-privileges:true

  # --------------------------------------------------------------------------
  # #29 — LiteLLM (AI Gateway / Model Router)
  # --------------------------------------------------------------------------
  litellm:
    image: ghcr.io/berriai/litellm:${LITELLM_TAG:-main-stable}
    container_name: mcp-litellm
    restart: unless-stopped
    networks:
      - mcp-ai-net
      - mcp-app-net
    expose:
      - "4000"
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}
    command: ["--config", "/app/config.yaml"]
    volumes:
      - ../../config/ai/models.yml:/app/config.yaml:ro
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:4000/health/liveliness', timeout=3).read()"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 30s
    depends_on:
      ollama:
        condition: service_started
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
    logging: *default-logging
    security_opt:
      - no-new-privileges:true

  # --------------------------------------------------------------------------
  # #30 — LangChain Worker (RAG Pipeline)
  # --------------------------------------------------------------------------
  langchain:
    image: mcp-langchain:latest
    container_name: mcp-langchain
    restart: unless-stopped
    pull_policy: never
    build:
      context: ../../containers/langchain-worker
      dockerfile: Dockerfile
    networks:
      - mcp-ai-net
      - mcp-data-net
      - mcp-app-net
    environment:
      PGVECTOR_HOST: pgvector
      PGVECTOR_PORT: 5432
      PGVECTOR_USER: ${PGVECTOR_USER}
      PGVECTOR_PASSWORD: ${PGVECTOR_PASSWORD}
      PGVECTOR_DB: ${PGVECTOR_DB}
      OLLAMA_HOST: http://ollama:11434
      LITELLM_HOST: http://litellm:4000
      REDIS_QUEUE_HOST: redis-queue
      REDIS_QUEUE_PORT: 6379
      EMBEDDING_MODEL: ${EMBEDDING_MODEL:-nomic-embed-text}
      PRIMARY_MODEL: ${OLLAMA_MODEL:-mistral:7b}
      ZAMMAD_URL: http://zammad-rails:3000
      ZAMMAD_TOKEN: ${ZAMMAD_AI_TOKEN}
      NTFY_URL: http://ntfy:80
    tmpfs:
      - /tmp:size=64m
    volumes:
      - ../../config/ai/prompts:/app/config/prompts:ro
      - ../../config/ai/rag-config.yml:/app/config/rag-config.yml:ro
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'python.*worker' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s
    depends_on:
      ollama:
        condition: service_started
      redis-queue:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"
    logging: *default-logging
    security_opt:
      - no-new-privileges:true

  # --------------------------------------------------------------------------
  # #31 — AI Gateway (Central AI API — FastAPI)
  # RULE: NEVER expose to public — internal networks only
  # --------------------------------------------------------------------------
  ai-gateway:
    image: mcp-ai-gateway:latest
    container_name: mcp-ai-gateway
    restart: unless-stopped
    pull_policy: never
    build:
      context: ../../containers/ai-gateway
      dockerfile: Dockerfile
    networks:
      - mcp-ai-net
      - mcp-app-net
      - mcp-data-net
    expose:
      - "8000"
    environment:
      AI_GATEWAY_SECRET: ${AI_GATEWAY_SECRET}
      OLLAMA_HOST: http://ollama:11434
      LITELLM_HOST: http://litellm:4000
      PGVECTOR_HOST: pgvector
      PGVECTOR_PORT: 5432
      PGVECTOR_USER: ${PGVECTOR_USER}
      PGVECTOR_PASSWORD: ${PGVECTOR_PASSWORD}
      PGVECTOR_DB: ${PGVECTOR_DB}
      REDIS_QUEUE_HOST: redis-queue
      REDIS_QUEUE_PORT: 6379
      ZAMMAD_URL: http://zammad-rails:3000
      ZAMMAD_TOKEN: ${ZAMMAD_AI_TOKEN}
      NTFY_URL: http://ntfy:80
      PRIMARY_MODEL: ${OLLAMA_MODEL:-mistral:7b-instruct-v0.3-q4_K_M}
      EMBEDDING_MODEL: ${EMBEDDING_MODEL:-nomic-embed-text}
    tmpfs:
      - /tmp:size=64m
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:8000/health', timeout=3).read()"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      ollama:
        condition: service_started
      redis-queue:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1.0"
    logging: *default-logging
    security_opt:
      - no-new-privileges:true

  # --------------------------------------------------------------------------
  # #32 — Redis Queue (Dedicated AI Job Queue)
  # RULE: Separate from mcp-redis (#2)! This is AI-only.
  # --------------------------------------------------------------------------
  redis-queue:
    image: redis:${REDIS_QUEUE_TAG:-7-alpine}
    container_name: mcp-redis-queue
    restart: unless-stopped
    networks:
      - mcp-ai-net
    expose:
      - "6379"
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - mcp-redis-queue-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 15s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.25"
    logging: *default-logging
    security_opt:
      - no-new-privileges:true

# ============================================================================
# Networks
# ============================================================================
networks:
  mcp-ai-net:
    external: true
  mcp-app-net:
    external: true
  mcp-data-net:
    external: true

# ============================================================================
# Volumes
# ============================================================================
volumes:
  mcp-ollama-data:
    external: true
  mcp-redis-queue-data:
    external: true
