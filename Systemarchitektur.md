# MCP v7 ‚Äî Projektplan & Systemarchitektur

> **Dieses Dokument ist die zentrale Wahrheit des Projekts.**
> Jede KI (Claude, Copilot, etc.) die an diesem Projekt arbeitet, MUSS dieses Dokument zuerst lesen und verstehen, bevor Code geschrieben oder √Ñnderungen vorgenommen werden.

---

## 1. Vision

MCP ist ein **vollst√§ndig lokales, KI-gest√ºtztes IT-Operations-Center** ‚Äî ein modernes, sicheres System, das jede Firma haben will. Kein Internet. Kein Cloud. Alles l√§uft auf einer einzigen Maschine. Die KI denkt mit, erkennt Probleme bevor sie passieren, erstellt automatisch Tickets und lernt aus jeder gel√∂sten St√∂rung.

**Kernprinzipien:**

- **100% Offline** ‚Äî kein externer API-Call, kein Cloud-Dienst, kein Telemetrie-Abfluss
- **KI-First** ‚Äî jedes Signal (Log, Metrik, Alert) flie√üt durch die lokale KI-Pipeline
- **Zero-Trust intern** ‚Äî Netzwerksegmentierung, Secrets-Management, MFA
- **Human-in-the-Loop** ‚Äî KI schl√§gt vor, Mensch entscheidet (konfigurierbar)
- **Ein Skript, ein Befehl** ‚Äî Installation, Validierung und Betrieb √ºber ein einziges intelligentes Skript

---

## 2. Ziel-Hardware

| Komponente     | Spezifikation                    | Zweck                                    |
|----------------|----------------------------------|------------------------------------------|
| **RAM**        | 32 GB                            | 32 Container parallel                    |
| **Storage**    | 500 GB SSD/HDD                   | OS + Container + Daten + KI-Modelle      |
| **GPU**        | NVIDIA RTX 4070 (12 GB VRAM)     | Lokale LLM-Inferenz (Ollama)             |
| **OS**         | Debian 12 / Ubuntu 24.04         | Docker Host                              |
| **Netzwerk**   | Statische lokale IP (z.B. `192.168.1.100`) | Alle Dienste √ºber HTTP erreichbar |

### RAM-Verteilung (gesch√§tzt)

| Stack        | Container | RAM (ca.) |
|--------------|-----------|-----------|
| Core         | 8         | ~5 GB     |
| Ops          | 8         | ~4 GB     |
| Telemetry    | 8         | ~4 GB     |
| Remote       | 3         | ~1 GB     |
| AI           | 5         | ~12 GB    |
| System/OS    | ‚Äî         | ~4 GB     |
| **Reserve**  | ‚Äî         | ~2 GB     |
| **Gesamt**   | **32**    | **~32 GB**|

### GPU-Nutzung

- **Ollama** mit NVIDIA Container Toolkit (`nvidia-docker`)
- Modell: `mistral:7b-instruct-v0.3` oder `llama3:8b` (passt in 12 GB VRAM)
- Quantisierung: Q4_K_M f√ºr optimale Geschwindigkeit/Qualit√§t
- Fallback: CPU-Inferenz wenn GPU nicht verf√ºgbar

---

## 3. Netzwerk-Architektur (5 isolierte Netze)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PHYSISCHES NETZWERK                       ‚îÇ
‚îÇ                   192.168.1.0/24 (LAN)                      ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ HOST: 192.168.1.100 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ mcp-edge-net (172.20.0.0/24) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  nginx (Reverse Proxy + Dashboard)            ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Port 80/443 ‚Üí 192.168.1.100                  ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ mcp-app-net (172.20.1.0/24) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  keycloak, n8n, zammad, bookstack,           ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  vaultwarden, meshcentral, guacamole,        ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  portainer, ntfy, uptime-kuma                ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ mcp-data-net (172.20.2.0/24) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  postgresql, redis, pgvector, elasticsearch  ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ mcp-sec-net (172.20.3.0/24) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  openbao (Secrets), crowdsec (IDS)           ‚îÇ     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ                                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ mcp-ai-net (172.20.4.0/24) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ollama (GPU), litellm, langchain-worker,   ‚îÇ      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ai-gateway, redis-queue, pgvector (RAG)    ‚îÇ      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚ö† NIEMALS nach au√üen exponieren!            ‚îÇ      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Regel:** Nur `mcp-edge-net` ist vom LAN erreichbar. Alles andere ist intern.

---

## 4. Stack-√úbersicht (32 Container)

### 4.1 Core Stack ‚Äî 8 Container (Basis-Infrastruktur)

| #  | Container          | Image                  | Aufgabe                        | Netzwerk(e)              |
|----|--------------------|------------------------|--------------------------------|--------------------------|
| 1  | mcp-postgres       | postgres:16            | Zentrale Datenbank             | mcp-data-net             |
| 2  | mcp-redis          | redis:7                | Cache & Queue                  | mcp-data-net             |
| 3  | mcp-pgvector       | pgvector/pgvector:pg16 | Vektor-DB f√ºr RAG              | mcp-data-net, mcp-ai-net|
| 4  | mcp-openbao        | openbao/openbao        | Secrets Management             | mcp-sec-net, mcp-app-net|
| 5  | mcp-nginx          | nginx:alpine           | Reverse Proxy + Dashboard      | mcp-edge-net, mcp-app-net|
| 6  | mcp-keycloak       | keycloak/keycloak      | Identity & MFA                 | mcp-app-net, mcp-data-net|
| 7  | mcp-n8n            | n8nio/n8n              | Workflow-Automatisierung       | mcp-app-net, mcp-data-net|
| 8  | mcp-ntfy           | binwiederhier/ntfy     | Lokale Push-Benachrichtigungen | mcp-app-net              |

> **Neu: mcp-ntfy** ‚Äî Ohne Internet gibt es keinen E-Mail-Versand. ntfy ist ein lokaler Benachrichtigungsdienst: Zabbix-Alerts, KI-Ticket-Status und System-Warnungen werden als Push-Nachrichten an Browser/Mobile geliefert. Alle Dienste (n8n, Zabbix, Grafana) k√∂nnen √ºber eine einfache HTTP-POST-API Nachrichten senden.

### 4.2 Ops Stack ‚Äî 8 Container (Betrieb & Dokumentation)

| #  | Container              | Image                         | Aufgabe                    | Netzwerk(e)              |
|----|------------------------|-------------------------------|----------------------------|--------------------------|
| 9  | mcp-zammad-rails       | zammad/zammad                 | Ticketsystem (Web)         | mcp-app-net, mcp-data-net|
| 10 | mcp-zammad-websocket   | zammad/zammad                 | Ticketsystem (WebSocket)   | mcp-app-net              |
| 11 | mcp-zammad-worker      | zammad/zammad                 | Ticketsystem (Background)  | mcp-app-net, mcp-data-net|
| 12 | mcp-elasticsearch      | elasticsearch:8               | Zammad Volltextsuche       | mcp-data-net             |
| 13 | mcp-bookstack          | lscr.io/linuxserver/bookstack | Wiki & Dokumentation       | mcp-app-net, mcp-data-net|
| 14 | mcp-vaultwarden        | vaultwarden/server            | Passwort-Manager           | mcp-app-net              |
| 15 | mcp-portainer          | portainer/portainer-ce        | Docker-Management-UI       | mcp-app-net              |
| 16 | mcp-diun               | crazymax/diun                 | Docker Image Update Watcher| mcp-app-net              |

> **Neu: mcp-portainer** ‚Äî Bei 32 Containern ist ein visuelles Docker-Management unverzichtbar. Portainer zeigt Container-Status, Logs, Ressourcenverbrauch, Netzwerke und Volumes in einer Web-UI. Erreichbar √ºber `/portainer` im Dashboard.

> **Verschoben: mcp-diun** ‚Äî War f√§lschlicherweise im AI Stack. Diun √ºberwacht Docker-Images auf neue Versionen ‚Äî das ist eine Betriebsaufgabe, keine KI-Funktion. Im Offline-Betrieb pr√ºft Diun gegen eine optionale lokale Registry.

### 4.3 Telemetry Stack ‚Äî 8 Container (Monitoring, Logging & Sicherheitserkennung)

| #  | Container              | Image                          | Aufgabe                          | Netzwerk(e)              |
|----|------------------------|--------------------------------|----------------------------------|--------------------------|
| 17 | mcp-zabbix-server      | zabbix/zabbix-server-pgsql     | Monitoring-Engine                | mcp-app-net, mcp-data-net|
| 18 | mcp-zabbix-web         | zabbix/zabbix-web-nginx-pgsql  | Zabbix Web-UI                    | mcp-app-net              |
| 19 | mcp-grafana            | grafana/grafana                | Dashboards & Visualisierung      | mcp-app-net, mcp-data-net|
| 20 | mcp-loki               | grafana/loki                   | Log-Aggregation                  | mcp-data-net             |
| 21 | mcp-alloy              | grafana/alloy                  | Log/Metrik-Collector             | mcp-app-net, mcp-data-net|
| 22 | mcp-uptime-kuma        | louislam/uptime-kuma           | Verf√ºgbarkeits-Monitoring        | mcp-app-net              |
| 23 | mcp-crowdsec           | crowdsecurity/crowdsec         | Intrusion Detection (IDS)        | mcp-sec-net, mcp-app-net|
| 24 | mcp-grafana-renderer   | grafana/grafana-image-renderer | PDF/Bild-Export f√ºr Berichte     | mcp-app-net              |

> **Neu: mcp-crowdsec** ‚Äî Lokale Intrusion Detection. CrowdSec analysiert Logs (nginx, SSH, Zabbix) und erkennt Angriffsmuster: Brute-Force, Port-Scans, SQL-Injection-Versuche. Im Offline-Modus arbeitet es mit lokalen Szenarien (keine Cloud-Blocklisten). Erkannte Bedrohungen werden an die KI-Pipeline weitergeleitet ‚Üí automatisches Sicherheits-Ticket.

> **Neu: mcp-grafana-renderer** ‚Äî Generiert PDF- und PNG-Exports von Grafana-Dashboards. N√ºtzlich f√ºr automatisierte Tagesberichte, die per ntfy oder Zammad verteilt werden. Ohne Renderer kann Grafana keine Bilder f√ºr Benachrichtigungen erzeugen.

### 4.4 Remote Stack ‚Äî 3 Container (Fernwartung)

| #  | Container          | Image                  | Aufgabe                        | Netzwerk(e)              |
|----|--------------------|------------------------|--------------------------------|--------------------------|
| 25 | mcp-meshcentral    | meshcentral/meshcentral| Remote Desktop & Management    | mcp-app-net              |
| 26 | mcp-guacamole      | guacamole/guacamole    | Web-basierter Remote-Zugang    | mcp-app-net              |
| 27 | mcp-guacd          | guacamole/guacd        | Guacamole Proxy Daemon         | mcp-app-net              |

### 4.5 AI Stack ‚Äî 5 Container (Lokale KI-Engine)

| #  | Container          | Image                  | Aufgabe                           | Netzwerk(e)              |
|----|--------------------|------------------------|-----------------------------------|--------------------------|
| 28 | mcp-ollama         | ollama/ollama          | LLM-Inferenz (GPU)               | mcp-ai-net               |
| 29 | mcp-litellm        | ghcr.io/berriai/litellm| KI-Gateway / Router              | mcp-ai-net, mcp-app-net  |
| 30 | mcp-langchain      | custom build           | KI-Pipeline Worker                | mcp-ai-net, mcp-data-net |
| 31 | mcp-ai-gateway     | custom build           | API f√ºr interne KI-Anfragen       | mcp-ai-net, mcp-app-net  |
| 32 | mcp-redis-queue    | redis:7-alpine         | Dedizierte Job-Queue f√ºr KI       | mcp-ai-net               |

> **Neu: mcp-redis-queue** ‚Äî Separater Redis speziell f√ºr die KI-Job-Queue. Isoliert KI-Workloads vom Haupt-Redis (mcp-redis), damit KI-Batch-Verarbeitung den Cache der Kernservices nicht beeintr√§chtigt. Speichert: Pending-Alerts, Analyse-Ergebnisse, RAG-Anfragen in der Warteschlange.

### Container-Gesamt√ºbersicht

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STACK          ‚îÇ  CONTAINER  ‚îÇ  RAM (ca.)  ‚îÇ  NUMMERN    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Core           ‚îÇ     8       ‚îÇ    ~5 GB    ‚îÇ   #1 ‚Äì #8   ‚îÇ
‚îÇ  Ops            ‚îÇ     8       ‚îÇ    ~4 GB    ‚îÇ   #9 ‚Äì #16  ‚îÇ
‚îÇ  Telemetry      ‚îÇ     8       ‚îÇ    ~4 GB    ‚îÇ  #17 ‚Äì #24  ‚îÇ
‚îÇ  Remote         ‚îÇ     3       ‚îÇ    ~1 GB    ‚îÇ  #25 ‚Äì #27  ‚îÇ
‚îÇ  AI             ‚îÇ     5       ‚îÇ   ~12 GB    ‚îÇ  #28 ‚Äì #32  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  System/OS      ‚îÇ     ‚Äî       ‚îÇ    ~4 GB    ‚îÇ      ‚Äî      ‚îÇ
‚îÇ  Reserve        ‚îÇ     ‚Äî       ‚îÇ    ~2 GB    ‚îÇ      ‚Äî      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  GESAMT         ‚îÇ    32       ‚îÇ   ~32 GB    ‚îÇ   #1 ‚Äì #32  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 5. Datenfluss ‚Äî Alles flie√üt zur KI

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     SIGNALQUELLEN                                ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  üñ®Ô∏è Drucker ‚îÄ‚îÄ‚îê                                                 ‚îÇ
‚îÇ  üñ•Ô∏è Server ‚îÄ‚îÄ‚îÄ‚î§                                                 ‚îÇ
‚îÇ  üåê DNS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  üì° DHCP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Zabbix  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ   n8n    ‚îÇ              ‚îÇ
‚îÇ  üîí Security ‚îÄ‚î§     ‚îÇ (Monitor)‚îÇ     ‚îÇ(Workflow)‚îÇ              ‚îÇ
‚îÇ  üìä Logs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ  üîî Alerts ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ               ‚îÇ                     ‚îÇ
‚îÇ  üõ°Ô∏è CrowdSec ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ                     ‚îÇ
‚îÇ                                           ‚ñº                     ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ                    ‚îÇ   Grafana   ‚îÇ  ‚îÇ KI-Pipeline‚îÇ               ‚îÇ
‚îÇ                    ‚îÇ   + Loki    ‚îÇ  ‚îÇ (LangChain)‚îÇ               ‚îÇ
‚îÇ                    ‚îÇ (Dashboard) ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ                     ‚îÇ
‚îÇ                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ                                    ‚îÇ redis-queue ‚îÇ              ‚îÇ
‚îÇ                                    ‚îÇ (Job-Queue) ‚îÇ              ‚îÇ
‚îÇ                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                                          ‚ñº                     ‚îÇ
‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ                              ‚îÇ  Ollama (RTX     ‚îÇ               ‚îÇ
‚îÇ                              ‚îÇ  4070 / lokal)   ‚îÇ               ‚îÇ
‚îÇ                              ‚îÇ                  ‚îÇ               ‚îÇ
‚îÇ                              ‚îÇ ‚Ä¢ Analyse        ‚îÇ               ‚îÇ
‚îÇ                              ‚îÇ ‚Ä¢ Root Cause     ‚îÇ               ‚îÇ
‚îÇ                              ‚îÇ ‚Ä¢ Ticket-Entwurf ‚îÇ               ‚îÇ
‚îÇ                              ‚îÇ ‚Ä¢ Vorhersage     ‚îÇ               ‚îÇ
‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                                       ‚îÇ                        ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ                    ‚ñº          ‚ñº              ‚ñº                  ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ              ‚îÇ  Zammad   ‚îÇ ‚îÇ  pgvector   ‚îÇ ‚îÇ  ntfy ‚îÇ             ‚îÇ
‚îÇ              ‚îÇ (Tickets) ‚îÇ ‚îÇ (Wissen/RAG)‚îÇ ‚îÇ(Push) ‚îÇ             ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.1 Praxis-Beispiel: Drucker-√úberwachung

**Szenario:** Netzwerkdrucker `192.168.1.50` hat Papierstau

1. **Zabbix** erkennt SNMP-Trap oder Status-√Ñnderung vom Drucker
2. **n8n** Workflow wird getriggert: ‚ÄûDrucker-Alert empfangen"
3. **Kontexterfassung**: n8n sammelt letzte Metriken, Toner-Status, Fehlerhistorie
4. **Job-Queue**: Alert wird in `mcp-redis-queue` eingestellt zur KI-Verarbeitung
5. **RAG-Abruf**: pgvector findet √§hnliche Drucker-Incidents aus der Vergangenheit
6. **KI-Analyse**: Ollama bewertet ‚Üí ‚ÄûPapierstau, Fach 2, wiederkehrend seit 3 Tagen"
7. **Ticket**: Zammad-Ticket wird automatisch erstellt:
   - Titel: `[DRUCKER] HP LaserJet 402 - Wiederkehrender Papierstau Fach 2`
   - Priorit√§t: Mittel
   - Kategorie: Hardware ‚Üí Drucker
   - Empfohlene Ma√ünahme: ‚ÄûEinzugsrolle Fach 2 pr√ºfen/reinigen"
   - Historien-Referenz: Link zu letztem √§hnlichen Incident
8. **Benachrichtigung**: ntfy sendet Push-Nachricht an zust√§ndigen Techniker
9. **Wissen**: Nach L√∂sung ‚Üí Ticket wird in RAG-Datenbank eingespeist

### 5.2 Was wird √ºberwacht?

| Bereich        | Was                                          | Wie                              | ‚Üí KI-Aktion                        |
|----------------|----------------------------------------------|----------------------------------|-------------------------------------|
| **Drucker**    | Status, Toner, Papierstau, Seitenz√§hler      | SNMP √ºber Zabbix                 | Ticket + Vorhersage Tonerwechsel    |
| **Security**   | Failed Logins, Port-Scans, Cert-Ablauf       | CrowdSec + Zabbix + Loki        | Sicherheits-Ticket + Risikobewertung|
| **DNS intern** | Aufl√∂sung, Latenzen, Fehler                  | Zabbix DNS-Checks               | Alert bei Anomalien                  |
| **DHCP**       | Lease-Pool, Konflikte, unbekannte Ger√§te     | Zabbix + Log-Parsing            | Ticket bei Pool-Ersch√∂pfung         |
| **Server**     | CPU, RAM, Disk, Services, Docker-Health       | Zabbix Agent + Alloy            | Proaktives Ticket vor Ausfall        |
| **Netzwerk**   | Ping, Bandbreite, Paketverlust               | Zabbix + Uptime-Kuma            | Eskalation bei Ausfall               |
| **Container**  | Health, Restart-Count, Logs, Resource-Usage   | Portainer + Alloy ‚Üí Loki        | Auto-Restart oder Ticket             |
| **Zertifikate**| Ablaufdatum TLS/SSL                          | Zabbix + Uptime-Kuma            | Ticket 30/14/7 Tage vor Ablauf       |
| **Intrusion**  | Brute-Force, Scans, Anomalien                | CrowdSec ‚Üí Loki ‚Üí KI            | Sofort-Ticket + IP-Block             |

---

## 6. Einheitliches Dashboard (HTTP)

### Zugang

```
http://192.168.1.100            ‚Üí MCP Dashboard (Hauptseite)
http://192.168.1.100/grafana    ‚Üí Grafana (Metriken & Logs)
http://192.168.1.100/tickets    ‚Üí Zammad (Ticketsystem)
http://192.168.1.100/monitor    ‚Üí Zabbix (Monitoring)
http://192.168.1.100/wiki       ‚Üí BookStack (Dokumentation)
http://192.168.1.100/status     ‚Üí Uptime-Kuma (Verf√ºgbarkeit)
http://192.168.1.100/vault      ‚Üí Vaultwarden (Passw√∂rter)
http://192.168.1.100/auth       ‚Üí Keycloak (Identity/SSO)
http://192.168.1.100/auto       ‚Üí n8n (Automatisierung)
http://192.168.1.100/remote     ‚Üí MeshCentral (Fernwartung)
http://192.168.1.100/guac       ‚Üí Guacamole (Remote Desktop)
http://192.168.1.100/portainer  ‚Üí Portainer (Docker-Management)
http://192.168.1.100/notify     ‚Üí ntfy (Benachrichtigungen)
```

### Dashboard-Startseite

Die nginx-Startseite (`/`) zeigt ein einheitliches Dashboard mit:

- **System-Gesundheit**: alle 32 Container gr√ºn/gelb/rot (Daten von Portainer API)
- **Letzte KI-Aktionen**: die letzten 10 KI-generierten Tickets
- **Aktive Alarme**: aus Zabbix + Uptime-Kuma + CrowdSec
- **KI-Konfidenz-√úbersicht**: High/Medium/Low Verteilung
- **Schnellzugriff**: Kacheln zu allen 13 Diensten
- **Drucker-Status**: Live-√úbersicht aller √ºberwachten Drucker
- **Sicherheitslage**: CrowdSec-Zusammenfassung (blockierte IPs, Angriffsmuster)
- **Letzte Benachrichtigungen**: ntfy-Feed der letzten Push-Nachrichten

---

## 7. Installations-Skript (`mcp-install.sh`)

### Philosophie

> **Ein Befehl. Alles l√§uft. Oder es stoppt sofort mit klarem Fehlerprotokoll.**

```bash
sudo bash scripts/mcp-install.sh
```

### Phasen-Modell (Gate-System)

Das Skript durchl√§uft **6 Phasen**. Jede Phase hat einen **Gate-Check**. Wenn ein Gate fehlschl√§gt, stoppt das Skript sofort und schreibt die letzten 100 Log-Zeilen in eine Fehlerdatei.

```
Phase 1: Preflight     ‚Üí  Gate 1  ‚Üí  ‚úÖ oder ‚ùå STOP
Phase 2: Environment    ‚Üí  Gate 2  ‚Üí  ‚úÖ oder ‚ùå STOP
Phase 3: Core Stack     ‚Üí  Gate 3  ‚Üí  ‚úÖ oder ‚ùå STOP
Phase 4: Ops Stack      ‚Üí  Gate 4  ‚Üí  ‚úÖ oder ‚ùå STOP
Phase 5: Telemetry      ‚Üí  Gate 5  ‚Üí  ‚úÖ oder ‚ùå STOP
Phase 6: AI Stack       ‚Üí  Gate 6  ‚Üí  ‚úÖ oder ‚ùå STOP
```

### Phase 1: Preflight-Checks

```
‚ñ° Docker Engine installiert und l√§uft
‚ñ° Docker Compose Plugin vorhanden (v2+)
‚ñ° NVIDIA-Treiber installiert (nvidia-smi funktioniert)
‚ñ° NVIDIA Container Toolkit installiert
‚ñ° Mindestens 28 GB RAM frei
‚ñ° Mindestens 100 GB Speicher frei
‚ñ° Ben√∂tigte Ports frei (80, 443, 5432, 6379, ...)
‚ñ° .env Datei vorhanden und valide
‚ñ° Alle Docker-Images lokal vorhanden (kein Pull n√∂tig!)
‚ñ° GPU erkannt und nutzbar f√ºr Docker
```

### Phase 2: Environment-Setup

```
‚ñ° Docker-Netzwerke erstellt (5 Netze)
‚ñ° Docker-Volumes erstellt
‚ñ° Secrets generiert (Passw√∂rter, API-Keys)
‚ñ° Konfigurationsdateien generiert
‚ñ° nginx-Konfiguration mit lokaler IP + alle 13 Proxy-Pfade
‚ñ° SSL-Zertifikate generiert (Self-Signed f√ºr lokal)
‚ñ° /etc/hosts Eintr√§ge gesetzt
```

### Phase 3: Core Stack (8 Container: #1‚Äì#8)

```
‚ñ° PostgreSQL gestartet + Datenbanken erstellt
‚ñ° Redis gestartet + erreichbar
‚ñ° pgvector gestartet + Erweiterung geladen
‚ñ° OpenBao gestartet + initialized + unsealed
‚ñ° nginx gestartet + alle 13 Proxy-Pfade konfiguriert
‚ñ° Keycloak gestartet + Admin-Login funktioniert
‚ñ° n8n gestartet + Healthcheck OK
‚ñ° ntfy gestartet + Test-Nachricht gesendet + empfangen
```

### Phase 4: Ops Stack (8 Container: #9‚Äì#16)

```
‚ñ° Elasticsearch gestartet + Cluster gr√ºn
‚ñ° Zammad (Rails/WebSocket/Worker) gestartet + UI l√§dt
‚ñ° BookStack gestartet + Login-Seite erreichbar
‚ñ° Vaultwarden gestartet + Web-Vault erreichbar
‚ñ° Portainer gestartet + Docker-API verbunden + UI erreichbar
‚ñ° Diun gestartet + Container-Liste erkannt
```

### Phase 5: Telemetry Stack (8 Container: #17‚Äì#24)

```
‚ñ° Zabbix Server gestartet + verbunden mit PostgreSQL
‚ñ° Zabbix Web gestartet + Login m√∂glich
‚ñ° Grafana gestartet + Datasources konfiguriert (Loki, Zabbix, PostgreSQL)
‚ñ° Loki gestartet + /ready = OK
‚ñ° Alloy gestartet + Logs flie√üen zu Loki
‚ñ° Uptime-Kuma gestartet + Dashboard erreichbar
‚ñ° CrowdSec gestartet + Szenarien geladen + nginx-Bouncer aktiv
‚ñ° Grafana-Renderer gestartet + Test-Render erfolgreich
```

### Phase 6: AI Stack (5 Container: #28‚Äì#32)

```
‚ñ° Redis-Queue gestartet + erreichbar auf mcp-ai-net
‚ñ° Ollama gestartet + GPU erkannt (nvidia-smi im Container)
‚ñ° KI-Modell geladen (z.B. mistral:7b)
‚ñ° LiteLLM Gateway gestartet + /health OK
‚ñ° LangChain Worker gestartet + verbunden mit pgvector + redis-queue
‚ñ° AI Gateway gestartet + Test-Prompt beantwortet
‚ñ° n8n KI-Workflows aktiviert
‚ñ° Ende-zu-Ende Test: Fake-Alert ‚Üí redis-queue ‚Üí KI-Analyse ‚Üí Ticket in Zammad ‚Üí ntfy-Push
```

### Fehlerbehandlung

Wenn **irgendein Gate fehlschl√§gt**:

1. Skript stoppt **sofort** ‚Äî kein Weiter zur n√§chsten Phase
2. Fehlerprotokoll wird geschrieben nach: `logs/mcp-install-error-<TIMESTAMP>.log`
3. Inhalt des Fehlerprotokolls:
   - Welche Phase fehlgeschlagen ist
   - Welcher Gate-Check fehlgeschlagen ist
   - Die letzten **100 Zeilen** aus den relevanten Container-Logs
   - Docker-Status aller Container (`docker ps -a`)
   - System-Resourcen (RAM, Disk, GPU)
   - Vorgeschlagene L√∂sung (wenn bekannt)
4. Zusammenfassung wird auf dem Terminal angezeigt:

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚ùå  MCP INSTALLATION GESTOPPT                              ‚ïë
‚ïë                                                              ‚ïë
‚ïë  Phase:     4 ‚Äî Ops Stack                                    ‚ïë
‚ïë  Gate:      Zammad UI l√§dt nicht                             ‚ïë
‚ïë  Ursache:   Assets 404 ‚Äî RAILS_SERVE_STATIC_FILES fehlt     ‚ïë
‚ïë                                                              ‚ïë
‚ïë  Fehlerlog: logs/mcp-install-error-20260217-143022.log       ‚ïë
‚ïë  Container: docker logs mcp-zammad-rails --tail 100          ‚ïë
‚ïë                                                              ‚ïë
‚ïë  ‚Üí Fehler korrigieren, dann erneut starten:                  ‚ïë
‚ïë    sudo bash scripts/mcp-install.sh --resume-from phase4     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

### Wiederaufnahme

```bash
# Ab der fehlgeschlagenen Phase weitermachen:
sudo bash scripts/mcp-install.sh --resume-from phase4

# Komplett neu starten:
sudo bash scripts/mcp-install.sh --clean

# Nur bestimmte Phase testen:
sudo bash scripts/mcp-install.sh --only phase6
```

---

## 8. Sicherheitsarchitektur

### Netzwerk-Segmentierung

```
Internet ‚îÄ‚îÄ‚îÄ‚îÄ ‚úò BLOCKIERT ‚úò ‚îÄ‚îÄ‚îÄ‚îÄ MCP Host
                                    ‚îÇ
LAN (192.168.1.0/24) ‚îÄ‚îÄ‚îÄ‚îÄ nur Port 80/443 ‚îÄ‚îÄ‚îÄ‚îÄ mcp-edge-net
                                                     ‚îÇ
                                              mcp-app-net (intern)
                                                     ‚îÇ
                                              mcp-data-net (DB only)
                                                     ‚îÇ
                                              mcp-sec-net (Secrets + IDS)
                                                     ‚îÇ
                                              mcp-ai-net (KI, isoliert)
```

### Zugriffskontrolle

| Ebene           | Ma√ünahme                                              |
|-----------------|-------------------------------------------------------|
| **Netzwerk**    | 5 isolierte Docker-Bridge-Netze, kein Internet-Zugang |
| **Identity**    | Keycloak SSO mit MFA f√ºr alle Web-Dienste             |
| **Secrets**     | OpenBao f√ºr alle Passw√∂rter & API-Keys                |
| **Passw√∂rter**  | Auto-generiert, 32+ Zeichen, nie im Klartext          |
| **TLS**         | Self-Signed intern, alle Dienste √ºber HTTPS (nginx)   |
| **Container**   | Read-Only Filesystems wo m√∂glich, no-new-privileges   |
| **Logs**        | Alle Zugriffe geloggt ‚Üí Loki ‚Üí KI-Analyse             |
| **IDS**         | CrowdSec analysiert Logs lokal, erkennt Angriffsmuster|
| **KI-Gateway**  | Nur aus mcp-ai-net erreichbar, niemals √∂ffentlich     |

### Was die KI sicherheitstechnisch √ºberwacht

- Failed SSH/Login-Versuche ‚Üí CrowdSec-Alert ‚Üí KI-Analyse ‚Üí Ticket bei Schwellwert
- Neue/unbekannte Ger√§te im Netzwerk (DHCP) ‚Üí Ticket
- DNS-Anomalien (ungew√∂hnliche Abfragen) ‚Üí Ticket
- Zertifikate kurz vor Ablauf ‚Üí Ticket mit Countdown
- Container-Neustarts (CrashLoop) ‚Üí Portainer-Daten ‚Üí Analyse + Ticket
- Disk-/RAM-Trends ‚Üí Vorhersage + proaktives Ticket
- Port-Scan-Erkennung ‚Üí CrowdSec ‚Üí Sicherheits-Ticket sofort
- Brute-Force-Angriffe ‚Üí CrowdSec ‚Üí automatisches IP-Banning via nginx-Bouncer

---

## 9. KI-Konfiguration

### Modell-Strategie

```yaml
primary_model: "mistral:7b-instruct-v0.3-q4_K_M"
fallback_model: "llama3:8b-instruct-q4_K_M"
embedding_model: "nomic-embed-text"

inference:
  device: "cuda"          # RTX 4070
  gpu_layers: 35          # Alle Layer auf GPU
  context_window: 8192
  temperature: 0.1        # Niedrig f√ºr konsistente Analysen
  max_tokens: 2048

rag:
  vector_db: "pgvector"
  chunk_size: 512
  chunk_overlap: 50
  top_k: 5                # Top 5 √§hnliche Dokumente
  similarity_threshold: 0.7

queue:
  broker: "mcp-redis-queue"   # Dedizierte KI-Queue (nicht mcp-redis!)
  port: 6379
  db: 0
  max_concurrent_jobs: 3      # Max 3 parallele KI-Analysen
  job_timeout: 120            # 2 Minuten pro Analyse
  retry_on_failure: true
  max_retries: 2
```

### Prompt-Architektur

Jeder KI-Aufruf folgt einem strukturierten Prompt-Template:

```
SYSTEM: Du bist ein IT-Operations-Analyst. Analysiere den folgenden
        Alert und erstelle einen strukturierten Incident-Bericht.
        Antworte auf Deutsch. Sei pr√§zise und technisch korrekt.

KONTEXT:
- Alert-Typ: {alert_type}
- Quelle: {source}
- Zeitstempel: {timestamp}
- Aktuelle Metriken: {metrics}
- Letzte 10 Log-Zeilen: {logs}
- IDS-Daten: {crowdsec_alerts}

HISTORISCH (RAG):
- √Ñhnliche Incidents: {rag_results}

AUFGABE:
1. Root-Cause-Analyse (1-2 S√§tze)
2. Auswirkung (Gering/Mittel/Hoch/Kritisch)
3. Empfohlene Ma√ünahme (konkrete Schritte)
4. Konfidenz (High/Medium/Low)

FORMAT: JSON
```

### Konfidenz-Matrix

| Konfidenz | KI-Aktion                    | Mensch                   | Benachrichtigung        |
|-----------|------------------------------|--------------------------|-------------------------|
| **High**  | Ticket erstellen + zuweisen  | Wird benachrichtigt      | ntfy Push (Info)        |
| **Medium**| Ticket-Entwurf erstellen     | Muss pr√ºfen & freigeben  | ntfy Push (Aktion n√∂tig)|
| **Low**   | Nur Alert senden             | √úbernimmt komplett       | ntfy Push (Dringend)    |

---

## 10. Offline-Betrieb ‚Äî Kein Internet

### Was bedeutet ‚Äûkein Internet"?

- **Keine Docker-Pulls** zur Laufzeit ‚Üí alle Images vorher lokal gespeichert
- **Keine externen APIs** ‚Üí Ollama ersetzt OpenAI/Claude
- **Keine Cloud-Datenbanken** ‚Üí PostgreSQL + pgvector lokal
- **Keine externen NTP** ‚Üí lokaler NTP oder Host-Zeit
- **Keine Update-Checks** ‚Üí Diun nur f√ºr lokale Registry (optional)
- **Keine Telemetrie** ‚Üí kein Abfluss an Dritte
- **Keine Cloud-Blocklisten** ‚Üí CrowdSec arbeitet mit lokalen Szenarien
- **Keine E-Mail** ‚Üí ntfy ersetzt E-Mail-Benachrichtigungen lokal

### Vorbereitung f√ºr Offline-Deployment

```bash
# 1. Auf einer Maschine MIT Internet alle Images pullen:
bash scripts/mcp-pull-images.sh

# 2. Images als tar exportieren:
bash scripts/mcp-export-images.sh ‚Üí images/mcp-images-v7.tar.gz

# 3. Auf Zielmaschine (OHNE Internet) importieren:
bash scripts/mcp-import-images.sh images/mcp-images-v7.tar.gz

# 4. KI-Modelle separat exportieren:
bash scripts/mcp-export-models.sh ‚Üí models/ollama-models.tar.gz

# 5. CrowdSec-Szenarien lokal exportieren:
bash scripts/mcp-export-crowdsec.sh ‚Üí config/crowdsec/scenarios/

# 6. Dann Installation starten:
sudo bash scripts/mcp-install.sh
```

---

## 11. Dateistruktur

```
mcp-v7/
‚îú‚îÄ‚îÄ plan.md                          ‚Üê DIESES DOKUMENT
‚îú‚îÄ‚îÄ README.md                        ‚Üê Englische Projektbeschreibung
‚îú‚îÄ‚îÄ README.de.md                     ‚Üê Deutsche Projektbeschreibung
‚îú‚îÄ‚îÄ SECURITY.md                      ‚Üê Sicherheitsrichtlinie
‚îú‚îÄ‚îÄ .env.example                     ‚Üê Vorlage f√ºr Umgebungsvariablen
‚îú‚îÄ‚îÄ .env                             ‚Üê Lokale Konfiguration (NICHT committen!)
‚îú‚îÄ‚îÄ Makefile                         ‚Üê Shortcuts (make up, make down, etc.)
‚îÇ
‚îú‚îÄ‚îÄ compose/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml       ‚Üê #1‚Äì#8:  PostgreSQL, Redis, pgvector, OpenBao, nginx, Keycloak, n8n, ntfy
‚îÇ   ‚îú‚îÄ‚îÄ ops/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml       ‚Üê #9‚Äì#16: Zammad, Elasticsearch, BookStack, Vaultwarden, Portainer, Diun
‚îÇ   ‚îú‚îÄ‚îÄ telemetry/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml       ‚Üê #17‚Äì#24: Zabbix, Grafana, Loki, Alloy, Uptime-Kuma, CrowdSec, Grafana-Renderer
‚îÇ   ‚îú‚îÄ‚îÄ remote/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml       ‚Üê #25‚Äì#27: MeshCentral, Guacamole, Guacd
‚îÇ   ‚îî‚îÄ‚îÄ ai/
‚îÇ       ‚îî‚îÄ‚îÄ docker-compose.yml       ‚Üê #28‚Äì#32: Ollama, LiteLLM, LangChain, AI-Gateway, Redis-Queue
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ nginx/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nginx.conf               ‚Üê Haupt-Konfiguration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conf.d/                   ‚Üê 13 Proxy-Configs (grafana.conf, tickets.conf, portainer.conf, ...)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboard/               ‚Üê HTML/CSS/JS f√ºr Startseite
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasources.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboards/
‚îÇ   ‚îú‚îÄ‚îÄ loki/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loki-config.yml
‚îÇ   ‚îú‚îÄ‚îÄ zabbix/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ zabbix_agent2.conf
‚îÇ   ‚îú‚îÄ‚îÄ alloy/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.alloy
‚îÇ   ‚îú‚îÄ‚îÄ keycloak/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ realm-export.json
‚îÇ   ‚îú‚îÄ‚îÄ n8n/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ workflows/               ‚Üê KI-Pipeline Workflows
‚îÇ   ‚îú‚îÄ‚îÄ ntfy/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server.yml               ‚Üê ntfy-Konfiguration (Topics, Auth)
‚îÇ   ‚îú‚îÄ‚îÄ crowdsec/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ acquis.yml               ‚Üê Log-Quellen (nginx, SSH, etc.)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scenarios/               ‚Üê Lokale Erkennungsregeln
‚îÇ   ‚îú‚îÄ‚îÄ portainer/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ portainer-data/          ‚Üê Portainer Persistenz
‚îÇ   ‚îî‚îÄ‚îÄ ai/
‚îÇ       ‚îú‚îÄ‚îÄ prompts/                  ‚Üê Strukturierte Prompt-Templates
‚îÇ       ‚îú‚îÄ‚îÄ models.yml                ‚Üê LiteLLM Modell-Routing
‚îÇ       ‚îî‚îÄ‚îÄ rag-config.yml            ‚Üê RAG-Pipeline Konfiguration
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ mcp-install.sh               ‚Üê Haupt-Installationsskript (6 Phasen + Gates)
‚îÇ   ‚îú‚îÄ‚îÄ mcp-start.sh                 ‚Üê Orchestrierung (Start aller 5 Stacks in Reihenfolge)
‚îÇ   ‚îú‚îÄ‚îÄ mcp-stop.sh                  ‚Üê Geordnetes Herunterfahren (AI ‚Üí Remote ‚Üí Telemetry ‚Üí Ops ‚Üí Core)
‚îÇ   ‚îú‚îÄ‚îÄ mcp-status.sh                ‚Üê Status aller 32 Container (tabellarisch)
‚îÇ   ‚îú‚îÄ‚îÄ mcp-pull-images.sh           ‚Üê Alle 32 Images pullen (online)
‚îÇ   ‚îú‚îÄ‚îÄ mcp-export-images.sh         ‚Üê Images als tar exportieren
‚îÇ   ‚îú‚îÄ‚îÄ mcp-import-images.sh         ‚Üê Images aus tar importieren
‚îÇ   ‚îú‚îÄ‚îÄ mcp-export-models.sh         ‚Üê KI-Modelle exportieren
‚îÇ   ‚îú‚îÄ‚îÄ mcp-export-crowdsec.sh       ‚Üê CrowdSec-Szenarien exportieren
‚îÇ   ‚îú‚îÄ‚îÄ mcp-backup.sh                ‚Üê Vollst√§ndiges Backup
‚îÇ   ‚îú‚îÄ‚îÄ mcp-restore.sh               ‚Üê Wiederherstellung aus Backup
‚îÇ   ‚îî‚îÄ‚îÄ init-db.sh                   ‚Üê PostgreSQL Datenbank-Initialisierung
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ smoke-test.sh                ‚Üê Basis-Gesundheitscheck (alle 32 Container)
‚îÇ   ‚îú‚îÄ‚îÄ ai-pipeline-test.sh          ‚Üê KI Ende-zu-Ende Test (Alert ‚Üí Queue ‚Üí Analyse ‚Üí Ticket ‚Üí Push)
‚îÇ   ‚îî‚îÄ‚îÄ security-test.sh             ‚Üê Sicherheits-Validierung (CrowdSec + Netzwerktrennung)
‚îÇ
‚îú‚îÄ‚îÄ logs/                             ‚Üê Fehlerprotokolle
‚îÇ   ‚îî‚îÄ‚îÄ mcp-install-error-*.log
‚îÇ
‚îú‚îÄ‚îÄ images/                           ‚Üê Exportierte Docker-Images (f√ºr Offline)
‚îÇ   ‚îî‚îÄ‚îÄ mcp-images-v7.tar.gz
‚îÇ
‚îú‚îÄ‚îÄ models/                           ‚Üê Exportierte KI-Modelle (f√ºr Offline)
‚îÇ   ‚îî‚îÄ‚îÄ ollama-models.tar.gz
‚îÇ
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ assets/
    ‚îÇ   ‚îî‚îÄ‚îÄ mcp-logo.png
    ‚îú‚îÄ‚îÄ architektur.md
    ‚îú‚îÄ‚îÄ deployment-runbook.md
    ‚îî‚îÄ‚îÄ troubleshooting.md
```

---

## 12. Bekannte Fehlermuster & L√∂sungen

> Aus Phase 2 und Phase 4 Installationstests dokumentiert.

| # | Problem | Ursache | L√∂sung | Datei |
|---|---------|---------|--------|-------|
| 1 | PostgreSQL: `permission denied to create database` | Rolle ohne CREATEDB | `ALTER ROLE zammad CREATEDB;` in init-db.sh | scripts/init-db.sh |
| 2 | PostgreSQL: `database "mcp_admin" does not exist` | DB fehlt | `CREATE DATABASE mcp_admin OWNER mcp_admin;` | scripts/init-db.sh |
| 3 | Zammad: Assets 404, Ladebildschirm h√§ngt | `RAILS_SERVE_STATIC_FILES` fehlt | In compose/ops env hinzuf√ºgen | compose/ops/docker-compose.yml |
| 4 | Loki: CrashLoop `compactor.delete-request-store` | Retention ohne Compactor-Config | `retention_enabled: false` | config/loki/loki-config.yml |
| 5 | Zabbix: `fe_sendauth: no password supplied` | Bind-Mount √ºberschreibt env-Vars | Config-Bind-Mount entfernen | compose/telemetry/docker-compose.yml |
| 6 | n8n: Healthcheck `Connection refused` | localhost ‚Üí IPv6, start_period zu kurz | `127.0.0.1`, start_period 120s | compose/core/docker-compose.yml |
| 7 | Guacamole: 404 auf `/` | Context-Path ist `/guacamole/` | URL korrigieren | tests/smoke-test.sh |
| 8 | MeshCentral: Empty reply | HTTPS auf Port 4430, nicht HTTP | `https://` verwenden | tests/smoke-test.sh |
| 9 | Orphan Containers Warning | Kein einheitlicher Projektname | `COMPOSE_PROJECT_NAME=mcp` | .env |

### Design-Patterns (Fehler vermeiden)

1. **Kein Bind-Mount f√ºr Configs die env-Vars brauchen** ‚Äî Bind-Mounts √ºberschreiben alles
2. **Immer `127.0.0.1` statt `localhost`** in Healthchecks (IPv6-Problem)
3. **`start_period: 120s`** f√ºr Services mit Datenbank-Migrationen
4. **Context-Pfade pr√ºfen** bevor Healthchecks geschrieben werden
5. **Protokoll pr√ºfen** (HTTP vs HTTPS) bei jedem Service
6. **`COMPOSE_PROJECT_NAME=mcp`** immer in .env setzen
7. **Separate Redis-Instanzen** ‚Äî mcp-redis f√ºr Cache, mcp-redis-queue f√ºr KI-Jobs
8. **CrowdSec ohne Internet** ‚Äî `online_client.enabled: false` in config setzen

---

## 13. Erfolgskriterien

Das System gilt als **produktionsreif** wenn:

- [ ] Alle 32 Container laufen mit STATUS=healthy
- [ ] Kein Container startet neu (0 Restarts in 1 Stunde)
- [ ] Dashboard unter `http://<IP>` erreichbar und zeigt alle 13 Dienste
- [ ] Alle 13 Proxy-Pfade in nginx funktionieren (/grafana, /tickets, /portainer, ...)
- [ ] Portainer zeigt alle 32 Container mit korrektem Status
- [ ] Zabbix √ºberwacht mindestens ein Ger√§t (Drucker/Server)
- [ ] KI-Pipeline funktioniert: Fake-Alert ‚Üí redis-queue ‚Üí Analyse ‚Üí Ticket in Zammad
- [ ] ntfy-Benachrichtigung wird bei Ticket-Erstellung empfangen
- [ ] RAG funktioniert: Gel√∂stes Ticket wird bei √§hnlichem Alert gefunden
- [ ] CrowdSec erkennt simulierten Brute-Force-Angriff
- [ ] Kein einziger ausgehender Netzwerk-Request (verifiziert via tcpdump)
- [ ] Keycloak SSO funktioniert f√ºr Grafana + Zammad
- [ ] Backup + Restore erfolgreich getestet
- [ ] Installationsskript l√§uft fehlerfrei durch alle 6 Phasen

---

## 14. Regeln f√ºr KI-Assistenten

> **Wenn du (Claude, GPT, Copilot) an diesem Projekt arbeitest, halte dich an diese Regeln:**

1. **Lies `plan.md` zuerst** ‚Äî immer. Vor jedem Code.
2. **Kein Internet-Zugriff** ‚Äî schreibe keinen Code der externe APIs, CDNs oder Paketmanager braucht
3. **Docker-only** ‚Äî alles l√§uft in Containern, kein `apt install` auf dem Host (au√üer Docker/NVIDIA)
4. **GPU beachten** ‚Äî Ollama MUSS die RTX 4070 nutzen, kein CPU-Fallback im Normalbetrieb
5. **Gate-System respektieren** ‚Äî wenn ein Test fehlschl√§gt, nicht weitermachen
6. **Fehlerlog schreiben** ‚Äî immer die letzten 100 Zeilen bei Fehler
7. **Deutsch** ‚Äî Tickets, Logs und KI-Ausgaben auf Deutsch
8. **Sicherheit** ‚Äî keine Secrets in Code, keine Ports nach au√üen, kein Root wo nicht n√∂tig
9. **Compose-Struktur** ‚Äî 5 separate docker-compose.yml in compose/, nicht eine riesige Datei
10. **Testen** ‚Äî jede √Ñnderung muss durch den relevanten Gate-Check validiert werden
11. **32 Container** ‚Äî genau 32, nummeriert #1‚Äì#32, aufgeteilt in 5 Stacks (8+8+8+3+5)
12. **Zwei Redis** ‚Äî mcp-redis (#2, Cache) und mcp-redis-queue (#32, KI-Jobs) niemals verwechseln
13. **CrowdSec offline** ‚Äî `online_client.enabled: false`, nur lokale Szenarien
14. **ntfy statt E-Mail** ‚Äî alle Benachrichtigungen √ºber ntfy, niemals SMTP konfigurieren

---

<p align="center">
  <strong>MCP v7 ‚Äî Managed Control Platform</strong><br/>
  <em>Lokal. Sicher. Automatisiert. Intelligent.</em><br/><br/>
  <code>sudo bash scripts/mcp-install.sh</code>
</p>
