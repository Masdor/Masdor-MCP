# MCP v7 â€” Test_last.md (Repair Prompt fÃ¼r Claude Code)

> **Datum:** 19.02.2026
> **Getestet auf:** WSL2 Ubuntu + Docker Desktop (x86_64, 15 GB RAM)
> **Repo:** https://github.com/Masdor/Masdor-MCP â€” Commit `bfdfc56` (repair/R1)
> **Admin:** Moustafa | **Assistent:** Claude Code
> **Ziel:** Alle Fehler aus dem Live-Test reparieren. Claude Code soll dieses Dokument als Prompt nutzen, um gezielt die genannten Dateien zu patchen und einen sauberen Commit zu pushen.

---

## REGELN FÃœR CLAUDE CODE

1. **NUR die hier genannten Dateien Ã¤ndern**, es sei denn ein Fix erfordert eine Referenz-Ã„nderung in einer anderen Datei.
2. **Keine echten Secrets commiten** â€” nur `.env.example`, Code, Compose, Config, Scripts, Tests.
3. **Minimal & chirurgisch** â€” keine unnÃ¶tigen Umbauten, nur die dokumentierten Fixes.
4. **Nach dem Patch:** Alle Compose-Configs mÃ¼ssen validieren (`docker compose config -q`), Scripts mÃ¼ssen `bash -n` bestehen, und `make test` muss grÃ¼n sein.
5. **Commit-Message:** `repair(R2): nginx subpath routing + AI compose fixes + test stability (Test_last.md)`

---

## STATUS NACH TEST (Smoke Test Ergebnis)

```
PASS: 32 | FAIL: 0 | WARN: 1

WARN: #29 LiteLLM â€” running but not healthy (none)  â† kein Healthcheck definiert
```

**Alle 33 Container laufen.** Installer-Gates bestanden. Remote Stack manuell gestartet.

---

## KATEGORIE A â€” KRITISCH: Dashboard-Links funktionieren nicht ðŸ”´

### Problem

Das Dashboard unter `http://127.0.0.1/` lÃ¤dt korrekt und zeigt alle 12 Service-Tiles.
**ABER: Kein einziger Link funktioniert im Browser.** Das ist der Haupt-Bug.

Die Dashboard-Links verwenden relative Pfade (`/grafana/`, `/tickets/`, `/vault/`, etc.), nginx hat `location`-BlÃ¶cke fÃ¼r alle 12 Pfade, **aber die Proxy-Weiterleitung ist falsch konfiguriert** â€” es fehlen Subpath-Rewrites, X-Forwarded-Prefix Headers und App-seitige Base-URL Konfiguration.

### HTTP-Probe-Ergebnisse (Live-Test)

| Pfad | HTTP Status | Problem |
|------|-------------|---------|
| `/auth/` | 302 â†’ `/auth/admin/` â†’ Redirect-Loop | Keycloak weiÃŸ nicht, dass es unter `/auth/` lÃ¤uft. KC_HTTP_RELATIVE_PATH fehlt oder Proxy-Prefix fehlt |
| `/auto/` | **200 âœ…** | n8n funktioniert |
| `/grafana/` | **301 â†’ `/grafana/` (Loop!)** | Grafana hat `GF_SERVER_ROOT_URL` + `SERVE_FROM_SUB_PATH=true`, ABER nginx rewritet den Pfad nicht korrekt. Proxy_pass leitet `/grafana/` als `/` weiter â†’ Grafana redirectet zu `/grafana/` â†’ Endlosschleife |
| `/guac/` | **200 âœ…** | Guacamole funktioniert |
| `/monitor/` | **200 âœ…** | Zabbix Web funktioniert |
| `/notify/` | **200 âœ…** | ntfy funktioniert |
| `/portainer/` | **307 â†’ `/timeout.html`** | Portainer braucht `--base-url /portainer`. Ohne Subpath-Awareness redirectet Portainer zu seinem internen Timeout-Endpoint |
| `/remote/` | **302 â†’ `https://127.0.0.1:443/`** | MeshCentral redirectet zu HTTPS Root. Muss den Reverse-Proxy-Prefix kennen |
| `/status/` | **302 â†’ `/dashboard`** | Uptime Kuma redirectet zu `/dashboard` statt `/status/dashboard`. Braucht `BASE_PATH=/status` oder nginx Rewrite-Fixup |
| `/tickets/` | **200 âœ…** | Zammad funktioniert |
| `/vault/` | **404 Not Found** | Vaultwarden erreichbar (Container healthy), aber nginx proxy_pass leitet falsch weiter. Vaultwarden unterstÃ¼tzt kein natives Subpath-Routing â†’ muss Ã¼ber separaten Server-Block oder `rewrite` gelÃ¶st werden |
| `/wiki/` | **302 â†’ `/wiki/login`** | BookStack redirectet korrekt, sollte funktionieren. Braucht `APP_URL=http://${MCP_HOST_IP}/wiki` |

### Root Cause

Die aktuelle `config/nginx/conf.d/default.conf` verwendet einfaches `proxy_pass $upstream/;` fÃ¼r alle Location-BlÃ¶cke. Das funktioniert **nur** wenn:
1. Die App keine Subpath-Awareness braucht (selten) UND
2. Nginx den Prefix korrekt strippt (passiert hier nicht)

Was fehlt in **JEDER** Location:
- **`rewrite`-Regel** um den Prefix zu strippen bevor er an den Upstream geht
- **`X-Forwarded-Prefix`-Header** damit die App weiÃŸ, unter welchem Pfad sie lÃ¤uft
- **`proxy_redirect`-Regel** damit Upstream-Redirects korrekt umgeschrieben werden

### FIX A1 â€” Nginx `default.conf` komplett neu schreiben

**Datei:** `config/nginx/conf.d/default.conf`

Jede Location muss diesem Muster folgen:

```nginx
# Redirect /pfad â†’ /pfad/ (trailing slash)
location = /grafana { return 301 /grafana/; }

location /grafana/ {
    set $upstream_grafana http://grafana:3000;
    
    # Subpath-Prefix strippen
    rewrite ^/grafana/(.*) /$1 break;
    
    proxy_pass $upstream_grafana;
    
    # Standard-Headers
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
    proxy_set_header X-Forwarded-Prefix /grafana;
    
    # Upstream-Redirects umschreiben
    proxy_redirect / /grafana/;
    proxy_redirect http://grafana:3000/ /grafana/;
}
```

**WICHTIG:** FÃ¼r Grafana speziell gilt:
- Grafana hat bereits `GF_SERVER_ROOT_URL` und `GF_SERVER_SERVE_FROM_SUB_PATH=true` â†’ also darf der Rewrite NICHT den Pfad strippen! Grafana erwartet den vollen `/grafana/`-Pfad.
- Fix fÃ¼r Grafana: `proxy_pass $upstream_grafana;` OHNE trailing slash und OHNE rewrite.

Korrekte Grafana-Location:
```nginx
location = /grafana { return 301 /grafana/; }
location /grafana/ {
    set $upstream_grafana http://grafana:3000;
    proxy_pass $upstream_grafana;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
}
```

**SpezialfÃ¤lle pro Service:**

| Service | LÃ¶sung |
|---------|--------|
| **Grafana** (`/grafana/`) | KEIN rewrite (App kennt Subpath via GF_SERVER). `proxy_pass $upstream;` ohne trailing `/` |
| **Portainer** (`/portainer/`) | `rewrite ^/portainer/(.*) /$1 break;` + Container braucht `command: --base-url /portainer` |
| **Vaultwarden** (`/vault/`) | Vaultwarden unterstÃ¼tzt KEIN Subpath-Routing. Es gibt 2 Optionen: (1) Subpath mit rewrite + `DOMAIN=http://HOST/vault` â€” funktioniert fÃ¼r die WeboberflÃ¤che, aber NICHT fÃ¼r Bitwarden-Clients. (2) Eigener `server_name vault.localhost` Block. **Empfehlung:** Option 1 fÃ¼r den Dashboard-Zugriff mit rewrite `^/vault/(.*) /$1 break;` |
| **Keycloak** (`/auth/`) | Keycloak 26+ verwendet `KC_HTTP_RELATIVE_PATH=/auth`. Muss als Env-Variable im Container gesetzt werden + `proxy_pass $upstream;` ohne rewrite (Keycloak kennt seinen Pfad) |
| **BookStack** (`/wiki/`) | Braucht `APP_URL=http://${MCP_HOST_IP}/wiki` als Env-Variable. Nginx: rewrite `^/wiki/(.*) /$1 break;` + `proxy_redirect / /wiki/;` |
| **Uptime Kuma** (`/status/`) | Kein natives Subpath-Support. Nginx rewrite + `proxy_redirect / /status/;`. Alternative: `sub_filter`-basiertes URL-Rewriting |
| **MeshCentral** (`/remote/`) | MeshCentral erzwingt HTTPS-Redirect. Muss mit `--webrtc-config` oder Env `REVERSE_PROXY=true` + `REVERSE_PROXY_URL=http://HOST/remote` konfiguriert werden |

### FIX A2 â€” Container-Umgebungsvariablen fÃ¼r Subpath-Awareness

**Datei:** `compose/ops/docker-compose.yml`

Portainer hinzufÃ¼gen:
```yaml
portainer:
    # ... bestehende config ...
    command: --base-url /portainer
```

BookStack hinzufÃ¼gen:
```yaml
bookstack:
    environment:
      # ... bestehende vars ...
      APP_URL: http://${MCP_HOST_IP}/wiki
```

**Datei:** `compose/core/docker-compose.yml`

Keycloak hinzufÃ¼gen:
```yaml
keycloak:
    environment:
      # ... bestehende vars ...
      KC_HTTP_RELATIVE_PATH: /auth
```

**Datei:** `compose/telemetry/docker-compose.yml`

Grafana hat bereits korrekte Vars âœ… (GF_SERVER_ROOT_URL + SERVE_FROM_SUB_PATH).

### FIX A3 â€” Nginx Config als Host-Mount (Persistenz)

**Problem:** Aktuell ist die nginx-Config innerhalb des Container-Images. Ã„nderungen gehen bei Recreate verloren.

**Datei:** `compose/core/docker-compose.yml`

Nginx-Service muss Volume-Mount haben:
```yaml
nginx:
    volumes:
      - ../../config/nginx/conf.d/default.conf:/etc/nginx/conf.d/default.conf:ro
      - ../../config/nginx/dashboard:/usr/share/nginx/html/dashboard:ro
```

**PrÃ¼fen:** Ist das bereits so? Falls ja, wird der Fix in `config/nginx/conf.d/default.conf` automatisch bei `docker compose up -d --force-recreate nginx` wirksam.

---

## KATEGORIE B â€” AI Stack Compose Fixes ðŸŸ¡

### FIX B1 â€” LiteLLM: Entferne `platform: linux/amd64`

**Datei:** `compose/ai/docker-compose.yml`
**Service:** `litellm`

**Problem:** `platform: linux/amd64` in der aktuellen Compose-Datei verursacht auf manchen Systemen:
```
Error: image does not provide the specified platform (linux/amd64)
```
Obwohl das System x86_64 ist, liefert das litellm Image manchmal kein explizites Platform-Manifest.

**Fix:** Zeile `platform: linux/amd64` komplett entfernen. Docker wÃ¤hlt automatisch die richtige Plattform.

### FIX B2 â€” LiteLLM: Command-Syntax korrigieren

**Datei:** `compose/ai/docker-compose.yml`
**Service:** `litellm`

**Problem:** In der Repo-Version steht:
```yaml
command: ["litellm", "--config", "/app/config.yaml"]
```
Aber das Image hat bereits `litellm` als Entrypoint â†’ Ergebnis: `litellm litellm --config ...` â†’ Error:
```
Error: Got unexpected extra argument (litellm)
```
Container restartet endlos.

**Fix:**
```yaml
command: ["--config", "/app/config.yaml"]
```

### FIX B3 â€” LiteLLM: Image-Tag pinnen

**Datei:** `compose/ai/docker-compose.yml`
**Service:** `litellm`

**Problem:** `litellm:latest` ist instabil. Der Tag wechselt hÃ¤ufig und kann InkompatibilitÃ¤ten verursachen.

**Fix:**
```yaml
image: ghcr.io/berriai/litellm:${LITELLM_TAG:-main-stable}
```

Und in `.env.example` ergÃ¤nzen:
```
LITELLM_TAG=main-stable
```

### FIX B4 â€” LiteLLM: Healthcheck hinzufÃ¼gen

**Datei:** `compose/ai/docker-compose.yml`
**Service:** `litellm`

**Problem:** Smoke-Test zeigt `WARN: running but not healthy (none)` weil kein Healthcheck definiert ist.

**Fix:** Healthcheck mit Python-stdlib (kein curl nÃ¶tig):
```yaml
healthcheck:
    test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:4000/health/liveliness', timeout=3).read()"]
    interval: 30s
    timeout: 10s
    retries: 10
    start_period: 30s
```

### FIX B5 â€” Ollama: Healthcheck image-native machen

**Datei:** `compose/ai/docker-compose.yml`
**Service:** `ollama`

**Problem:** Aktueller Healthcheck nutzt `curl`, das im Ollama-Image nicht immer vorhanden ist:
```yaml
test: ["CMD-SHELL", "curl -sf http://127.0.0.1:11434/api/tags >/dev/null"]
```

**Fix:** Image-nativen Befehl verwenden:
```yaml
healthcheck:
    test: ["CMD", "ollama", "list"]
    interval: 30s
    timeout: 10s
    retries: 15
    start_period: 300s
```

### FIX B6 â€” AI Gateway: Healthcheck image-native machen

**Datei:** `compose/ai/docker-compose.yml`
**Service:** `ai-gateway`

**Problem:** Healthcheck nutzt `curl`, das im python:slim Image nicht vorhanden ist.

**Fix:** Python-stdlib verwenden:
```yaml
healthcheck:
    test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:8000/health', timeout=3).read()"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 30s
```

### FIX B7 â€” depends_on: service_healthy â†’ service_started

**Datei:** `compose/ai/docker-compose.yml`
**Services:** `litellm`, `langchain`, `ai-gateway`

**Problem:** `depends_on: ollama: condition: service_healthy` blockiert den gesamten AI-Stack wenn Ollama langsam startet. Da Ollama-Models groÃŸ sind (4+ GB), kann der Start Ã¼ber 5 Minuten dauern.

**Fix:** Alle `condition: service_healthy` auf Ollama Ã¤ndern zu `condition: service_started`. Die Services haben eigene Retry-Logik.

### FIX B8 â€” LiteLLM: Healthcheck im Repo hat curl

**Datei:** `compose/ai/docker-compose.yml`
**Service:** `litellm`

**Problem:** Der aktuell im Repo definierte litellm-Healthcheck nutzt curl:
```yaml
test: ["CMD-SHELL", "curl -sf http://127.0.0.1:4000/health > /dev/null"]
```

**Fix:** Durch Python-stdlib ersetzen (siehe FIX B4). Falls beides vorhanden, den curl-basierten entfernen.

---

## KATEGORIE C â€” Ops Stack Fixes ðŸŸ¡

### FIX C1 â€” Vaultwarden: Healthcheck auf `/healthcheck.sh`

**Datei:** `compose/ops/docker-compose.yml`
**Service:** `vaultwarden`

**Problem:** Im Repo steht ein curl-basierter Healthcheck, der im vaultwarden-Image nicht funktioniert.

**Fix:** Das vaultwarden/server Image enthÃ¤lt `/healthcheck.sh`:
```yaml
healthcheck:
    test: ["CMD", "/healthcheck.sh"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 60s
```

### FIX C2 â€” Portainer: Healthcheck deaktivieren

**Datei:** `compose/ops/docker-compose.yml`
**Service:** `portainer`

**Problem:** Portainer hat keinen zuverlÃ¤ssigen image-nativen Healthcheck. FÃ¼hrt zu "unhealthy"-Noise.

**Fix:**
```yaml
healthcheck:
    test: ["NONE"]
```

Und in `tests/smoke-test.sh` Portainer als `running` (nicht `healthy`) prÃ¼fen.

---

## KATEGORIE D â€” AI Pipeline Test Fix ðŸŸ¡

### FIX D1 â€” `tests/ai-pipeline-test.sh` komplett neu schreiben

**Problem:** Der aktuelle Test nutzt `curl` innerhalb von `docker exec mcp-ai-gateway`, aber das python:slim Image hat kein curl. AuÃŸerdem wurde der Test bei manuellen Heredoc-Pastes mehrfach korrumpiert.

**Anforderungen an den neuen Test:**
1. **NUR Python-stdlib** verwenden (`urllib.request`) â€” kein curl
2. **OpenAPI-Endpoint-Discovery**: `/openapi.json` lesen um den korrekten POST-Endpoint zu finden
3. **Mehrere Auth-Header-Varianten** testen: `Authorization: Bearer`, `X-API-Key`, kein Auth
4. **Unique Description** mit Timestamp, damit Dedup (15-Min-TTL) nie triggert
5. **Redis-Key-Pattern-Scan**: Nicht nur `mcp:job:*` suchen, sondern auch `*queue*` und generic `*`
6. **Fehler-Output**: Bei FAIL muss HTTP-Status + Body angezeigt werden (Debug-Info)

**Die korrekte Gateway-API laut `containers/ai-gateway/app/main.py`:**
- Endpoint: `POST /api/v1/analyze`
- Auth: `Authorization: Bearer <AI_GATEWAY_SECRET>` (wenn Secret gesetzt)
- Body: `{"source":"...","severity":"...","host":"...","description":"...","metrics":{},"logs":"..."}`
- Response: `{"status":"queued","job_id":"job_123_zabbix","message":"..."}`
- Redis Keys: `mcp:job:<job_id>` (Hash) + `mcp:queue:analyze` (List) + `mcp:dedup:*` (TTL)

**Wichtig:** Der AI_GATEWAY_SECRET muss dem Container als Environment-Variable Ã¼bergeben werden bei `docker exec`:
```bash
docker exec -e AI_GATEWAY_SECRET="${AI_GATEWAY_SECRET}" mcp-ai-gateway python - <<'PY'
```

---

## KATEGORIE E â€” Installer & Scripts ðŸŸ¢

### FIX E1 â€” Remote Stack im Installer einbauen

**Datei:** `scripts/mcp-install.sh`

**Problem:** Der Installer hat keine Phase fÃ¼r den Remote Stack. Container mussten manuell gestartet werden.

**Fix:** Phase 7 fÃ¼r Remote Stack hinzufÃ¼gen (nach AI Stack):
```bash
# Phase 7: Remote Stack
docker compose -p "$PROJECT" --env-file .env -f compose/remote/docker-compose.yml up -d
```

### FIX E2 â€” COMPOSE_IGNORE_ORPHANS setzen

**Datei:** `scripts/mcp-install.sh`

**Problem:** Jede Phase wirft Warnungen: `Found orphan containers [...] for this project.`
Das ist normal (mehrere Compose-Dateien, ein Project-Name), aber verunsichert Nutzer.

**Fix:** Am Anfang des Installers setzen:
```bash
export COMPOSE_IGNORE_ORPHANS=1
```

### FIX E3 â€” `scripts/gen-test-env.sh` im Repo

**Problem:** FÃ¼r WSL-Tests muss .env manuell generiert werden. Das Script existiert nicht im Repo.

**Fix:** Folgendes Script als `scripts/gen-test-env.sh` committen:
- `.env.example` â†’ `.env` kopieren
- `COMPOSE_PROJECT_NAME=mcpwsl` und `MCP_HOST_IP=127.0.0.1` fÃ¼r WSL setzen
- Alle `CHANGE_ME` Werte durch starke Random-Secrets ersetzen (via `python3 secrets`)
- `.env` auf `600` setzen, Owner = aufrufender User (nicht root, damit `make` lesen kann)
- `--force` Flag fÃ¼r Ãœberschreiben
- `--prod` Flag fÃ¼r root:root Ownership (Production)

---

## KATEGORIE F â€” Optimierungen (VorschlÃ¤ge) ðŸ”µ

### OPT F1 â€” `pull_policy: never` fÃ¼r Custom Images

**Datei:** `compose/ai/docker-compose.yml`
**Services:** `ai-gateway`, `langchain`

**Problem:** Docker versucht die Custom Images zu pullen bevor er sie baut:
```
pull access denied for mcp-langchain, repository does not exist
```
Danach baut er sie trotzdem. Aber die Fehlermeldung ist verwirrend.

**Fix:**
```yaml
ai-gateway:
    pull_policy: never
    build:
      context: ../../containers/ai-gateway
```

### OPT F2 â€” Security-Test: Dynamischer Projektname

**Datei:** `tests/security-test.sh`

**Problem:** Filter ist hardcoded auf `com.docker.compose.project=mcp`. Wenn Projektname anders ist (z.B. `mcpwsl`), findet der Test keine Container.

**Fix:** Projektname aus `.env` laden:
```bash
if [ -f .env ]; then set -a; source .env; set +a; fi
PROJECT="${COMPOSE_PROJECT_NAME:-mcp}"
# Dann: --filter "label=com.docker.compose.project=${PROJECT}"
```

### OPT F3 â€” CrowdSec: Persistentes Data-Volume

**Datei:** `compose/telemetry/docker-compose.yml`
**Service:** `crowdsec`

**PrÃ¼fen:** Hat CrowdSec ein Volume fÃ¼r `/var/lib/crowdsec/data`? Falls nicht:
```yaml
volumes:
    - mcp-crowdsec-data:/var/lib/crowdsec/data
```
Und Volume als `external: true` deklarieren. In `scripts/mcp-install.sh` Phase 2 erstellen.

### OPT F4 â€” Zabbix Web: mcp-data-net hinzufÃ¼gen

**Datei:** `compose/telemetry/docker-compose.yml`
**Service:** `zabbix-web`

**PrÃ¼fen:** Hat zabbix-web Zugang zum `mcp-data-net`? Es muss PostgreSQL erreichen kÃ¶nnen.
```yaml
networks:
    - mcp-app-net
    - mcp-data-net
```

### OPT F5 â€” .env.example: ES_JAVA_OPTS quoten

**Datei:** `.env.example`

**Problem:** `ES_JAVA_OPTS=-Xms512m -Xmx512m` ohne Quotes â†’ kann Parsing brechen.

**Fix:**
```
ES_JAVA_OPTS="-Xms512m -Xmx512m"
```

---

## ZUSAMMENFASSUNG: DATEIEN ZUM Ã„NDERN

| # | Datei | Fixes |
|---|-------|-------|
| 1 | `config/nginx/conf.d/default.conf` | A1: Komplettes Rewrite aller Location-BlÃ¶cke mit Subpath-Support |
| 2 | `compose/ai/docker-compose.yml` | B1-B8: LiteLLM platform/command/tag/healthcheck, Ollama healthcheck, AI-Gateway healthcheck, depends_on |
| 3 | `compose/ops/docker-compose.yml` | C1: Vaultwarden healthcheck, C2: Portainer healthcheck, A2: Portainer command, BookStack APP_URL |
| 4 | `compose/core/docker-compose.yml` | A2: Keycloak KC_HTTP_RELATIVE_PATH, A3: Nginx volume-mounts prÃ¼fen |
| 5 | `compose/telemetry/docker-compose.yml` | F3: CrowdSec volume, F4: Zabbix-Web Netzwerk |
| 6 | `tests/ai-pipeline-test.sh` | D1: Komplett neu schreiben (Python-stdlib, OpenAPI discovery, Debug-Output) |
| 7 | `tests/smoke-test.sh` | C2: Portainer als "running" statt "healthy" prÃ¼fen |
| 8 | `tests/security-test.sh` | F2: Dynamischer Projektname |
| 9 | `scripts/mcp-install.sh` | E1: Remote Stack Phase, E2: COMPOSE_IGNORE_ORPHANS |
| 10 | `scripts/gen-test-env.sh` | E3: Neues Script fÃ¼r automatische .env-Generierung |
| 11 | `.env.example` | B3: LITELLM_TAG, F5: ES_JAVA_OPTS Quotes |

---

## VALIDIERUNG NACH PATCH

```bash
# 1) Compose-Config validieren (muss 0 Errors)
for f in compose/*/docker-compose.yml; do
  docker compose --env-file .env -f "$f" config -q && echo "OK: $f" || echo "FAIL: $f"
done

# 2) Script-Syntax prÃ¼fen
bash -n scripts/mcp-install.sh
bash -n scripts/gen-test-env.sh
bash -n tests/smoke-test.sh
bash -n tests/ai-pipeline-test.sh
bash -n tests/security-test.sh

# 3) Clean Install + Tests (auf WSL)
./scripts/gen-test-env.sh --force
sudo bash scripts/mcp-install.sh --clean
make status
make test

# 4) Dashboard-Links prÃ¼fen
for p in auth auto grafana guac monitor notify portainer remote status tickets vault wiki; do
  echo "=== /$p/ ==="
  curl -sI "http://127.0.0.1/$p/" | head -3
done

# 5) Keine Crashes
docker ps -a --format "table {{.Names}}\t{{.Status}}" | egrep -i "exited|restarting" || echo "OK: no crashes"
```

---

## ERWARTETES ERGEBNIS NACH R2

```
Smoke Test:    PASS: 33 | FAIL: 0 | WARN: 0
AI Pipeline:   PASS: 5  | FAIL: 0
Security Test: PASS (dynamischer Projektname)
Dashboard:     Alle 12 Links funktionieren im Browser
```

---

## COMMIT

```bash
git add -A
git status  # Verify nur gewollte Dateien
git commit -m "repair(R2): nginx subpath routing + AI compose fixes + test stability (Test_last.md)"
git push origin repair/R2
```
